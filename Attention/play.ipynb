{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Simplified Self-Attention**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single Input Attention Calculation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = inputs[1] # Getting the second row, or the features of the token \"journey\"\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "\n",
    "# loop through the tokens in the sequence\n",
    "for i, x_i in enumerate(inputs):\n",
    "    # take the dot product of each token embedding vector\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "    \n",
    "    # Dot Product example:\n",
    "    # torch.Tensor([1,2,3,4,5]).dot(torch.Tensor([2,1,2,1,2]))\n",
    "    # (1*2)+(2*1)+(3*2)+(4*1)+(5*2) = tensor(24.)\n",
    "\n",
    "attn_scores_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the attention scores\n",
    "attn_scores_norm = attn_scores_2 / torch.sum(attn_scores_2)\n",
    "print(f\"Unnormailzed: {attn_scores_2}\")\n",
    "print(f\"Normailzed: {attn_scores_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But softmax is more desireable for normalization\n",
    "# Do note that there are underflow and overflow issues that come from this softmax implementation\n",
    "print(attn_scores_2.exp() / attn_scores_2.exp().sum())\n",
    "\n",
    "# This softmax implementation is preffered.\n",
    "print(attn_scores_2.softmax(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the updated embeddings\n",
    "inputs.T @ attn_scores_2.softmax(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full Attention Calculation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the attention scores for the query\n",
    "attn_scores = inputs @ inputs.T\n",
    "\n",
    "# normalize w/ softmax\n",
    "attn_weights = attn_scores.softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute vectors\n",
    "attn_weights @ inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Self-Attention**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] # second input token\n",
    "d_in = inputs.shape[1] # The input embedding size, d=3\n",
    "d_out = 2 # the output embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# We will use three matricies to project the embedded tokens, into:\n",
    "# query vector: What we are \"interested in\"\n",
    "W_query = nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "# key vector: What we have\n",
    "W_key = nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "# Value vector: what information to communicate if it is \"interesting\"\n",
    "W_value = nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = inputs @ W_query\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "qk_attn = query @ keys.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the key dimension\n",
    "d_k = keys.shape[1]\n",
    "# divide by sqrt if embedding dimension for scalling\n",
    "# And apply softmax\n",
    "qk_norm = (qk_attn * d_k**-0.5).softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qk_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qk_norm @ values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Putting it all together**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.randn(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.randn(d_in, d_out))\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        q = x @ self.W_query\n",
    "        k = x @ self.W_key\n",
    "        v = x @ self.W_value\n",
    "\n",
    "        # multiply query and keys\n",
    "        attn_scores = q @ k.T\n",
    "\n",
    "        # Scaled normalization\n",
    "        d_k = k.shape[1]\n",
    "        attn_weights = (attn_scores * d_k**-0.5).softmax(dim=-1)\n",
    "\n",
    "        # qk normalized matmul values to get output\n",
    "        out = attn_weights @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "self_attn = SelfAttention_v1(3, 2)\n",
    "\n",
    "self_attn(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    \"\"\"Implementation using nn.Linear instead of matrix multiplication\"\"\"\n",
    "    def __init__(self, d_in, d_out, bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.W_keys = nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=bias)\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        q = self.W_query(x)\n",
    "        k = self.W_keys(x)\n",
    "        v = self.W_value(x)\n",
    "\n",
    "        # mul q & k\n",
    "        attn_scores = q @ k.T\n",
    "        \n",
    "        # normalize\n",
    "        attn_weights = (attn_scores * k.shape[1]**-0.5).softmax(dim=-1)\n",
    "\n",
    "        out = attn_weights @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "attn_v2 = SelfAttention_v2(3, 2)\n",
    "attn_v2(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Causal Attention**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Decoder Only Attention\"\"\"\n",
    "    def __init__(self, d_in:int, d_out:int, bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=bias)\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        q: torch.Tensor = self.W_query(x)\n",
    "        k: torch.Tensor = self.W_key(x)\n",
    "        v: torch.Tensor = self.W_value(x)\n",
    "\n",
    "        # compute attn scores\n",
    "        attn_scores = q @ k.T\n",
    "\n",
    "        # Since we can only attend to what is previously shown\n",
    "        attn_mask = ~torch.ones(attn_scores.shape).tril().bool()\n",
    "        # We need to now mask with (-inf) so softmax only deals with what we have\n",
    "        masked_attn = attn_scores.masked_fill(attn_mask, -torch.inf)\n",
    "        # Now scaled norm\n",
    "        attn_weights = (masked_attn * k.shape[1]**-0.5).softmax(dim=-1)\n",
    "\n",
    "        out = attn_weights @ v\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "attn_v3 = CausalSelfAttention(3,2)\n",
    "attn_v3(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add in Dropout & Make it more compact**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    \"\"\"Decoder Only Attention\"\"\"\n",
    "    def __init__(self, d_in:int, d_out:int, context_length:int, dropout:float, bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        # this is really a mask but GPT-2 and hf have buffer\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        # create dimensions\n",
    "        _, num_tokens, _ = x.shape # (B, tokens, )\n",
    "        # print(num_tokens)\n",
    "        q: torch.Tensor = self.W_query(x)\n",
    "        k: torch.Tensor = self.W_key(x)\n",
    "        v: torch.Tensor = self.W_value(x)\n",
    "\n",
    "        # compute attn scores\n",
    "        attn_scores = q @ k.transpose(1,2)\n",
    "\n",
    "        # New, _ ops are in-place\n",
    "        # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  \n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores * keys.size(-1)**-0.5, dim=-1)\n",
    "\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        out = attn_weights @ v\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set an example number of tokens\n",
    "token_count = 6 \n",
    "inputs = torch.randn((token_count, 3))\n",
    "print(inputs)\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Multi-head Attention**\n",
    "\n",
    "Adding multiple heads is essentially stacking multiple layers of single attention heads.\n",
    "This creates multiple latent spaces, so the model can attend to different to different information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in:int, d_out:int, context_length:int, dropout:float, num_heads=1, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, bias) for _ in range(num_heads)]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set an example number of tokens\n",
    "token_count = 6\n",
    "inputs = torch.randn((token_count, 3))\n",
    "print(inputs)\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = inputs.size(-1), 4\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=4\n",
    ")\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs[0,:,:])\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Weight Splits**\n",
    "\n",
    "Instead of creating a module list of attention heads, we instead created one MultiHeaded Attention implementation,\n",
    "where we define one q,k & v weight matrix, and split then to obtain the separate heads.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **To better understand this, lets do two things**\n",
    "\n",
    "1. Initialize all of the layers, dimensions, etc in a class.\n",
    "2. Step through the forward pass peice by peice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionEX(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in:int, d_out:int, context_length:int, dropout:float, num_heads=1, bias=False):\n",
    "        super().__init__()\n",
    "        assert(d_out % num_heads == 0), \"The output dimension must be divisible by the number of heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # This reduces the projection dimention to match the desired output.\n",
    "\n",
    "        # Define q, k, v. Remember these...\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=bias)\n",
    "\n",
    "        self.o_proj = nn.Linear(d_out, d_out) # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Create mask for decoder block\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn((6,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the batch inputs \n",
    "x_batch = torch.stack([inputs, inputs], dim=0)\n",
    "print(f\"Batch Shape: {x_batch.shape}\")\n",
    "print(f\"There are {x_batch.shape[0]} exmaples of size {x_batch.shape[1]}x{x_batch.shape[2]} in the batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, context_length, d_in = x_batch.shape\n",
    "d_out = 2\n",
    "\n",
    "mha = MultiHeadAttentionEX(d_in, d_out, context_length, 0.0, num_heads=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_start of the forward pass_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward(x): Think of the forward pass starting here.\n",
    "x = x_batch\n",
    "\n",
    "b, num_tokens, d_in = x.shape\n",
    "\n",
    "# create the full k, q, v matricies\n",
    "keys = mha.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "queries = mha.W_query(x)\n",
    "values = mha.W_value(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_split the matricies using `.view()`_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Previous key dimension: {keys.shape}\")\n",
    "keys = keys.view(b, num_tokens, mha.num_heads, mha.head_dim)\n",
    "print(f\"Post split key dimension: {keys.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So what did we do here?**\n",
    "\n",
    "Using .view() a way to take an exisitng tensor and temporarily hold it in newly defined dimensions, we split the output dimension by the number of heads we have.\n",
    "\n",
    "_Example:_\n",
    "\n",
    "- Output dimension = 16 | Attention heads 2 | Input dimensions are 2x6x16 (batch, context_length, d_out)\n",
    "- New head dimension is 16 // 2 => 8 (Will explain // shortly)\n",
    "- So we now want the key matrix to look like this 2x6x(attn_heads)x(new_dim) => 2x6x2x8\n",
    "\n",
    "- So you can think of treating the last two dimesnions as \"one\".\n",
    "\n",
    "- What happens if we increase the number of attention heads to 4?\n",
    "- New head dimension is 16 // 4 => 4 (will explain // shortly)\n",
    "- So we have a key matric like 2x6x4x4 (Note how the last two dimensions == Output Dimension)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's rework the example using the example numbers**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our example inputs to have 8 tokens (context_length of 8) and have an input dimension of 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, in_dim = 8, 6\n",
    "inputs_v2 = torch.randn((tokens, in_dim))\n",
    "# Create the batch inputs \n",
    "x_batch = torch.stack([inputs_v2, inputs_v2], dim=0)\n",
    "print(f\"Batch Shape: {x_batch.shape}\")\n",
    "print(f\"There are {x_batch.shape[0]} exmaples of size {x_batch.shape[1]}x{x_batch.shape[2]} in the batch\")\n",
    "batch_size, context_length, d_in = x_batch.shape\n",
    "d_out, num_heads = 16, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, context_length, d_in = x_batch.shape\n",
    "d_out, num_heads = 16, 4\n",
    "\n",
    "mha = MultiHeadAttentionEX(d_in, d_out, context_length, 0.05, num_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward(x): Think of the forward pass starting here.\n",
    "x = x_batch\n",
    "\n",
    "b, num_tokens, d_in = x.shape\n",
    "\n",
    "# create the full k, q, v matricies\n",
    "keys:torch.Tensor = mha.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "queries:torch.Tensor = mha.W_query(x)\n",
    "values:torch.Tensor = mha.W_value(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_split the matricies_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Previous key dimension: {keys.shape}\")\n",
    "keys = keys.view(b, num_tokens, mha.num_heads, mha.head_dim)\n",
    "print(f\"Post split key dimension: {keys.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_do the same for the query and the values_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = queries.view(b, num_tokens, mha.num_heads, mha.head_dim)\n",
    "values = values.view(b, num_tokens, mha.num_heads, mha.head_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Now we need to Transpose the key matrix_\n",
    "\n",
    "Wait why are we doing this now?\n",
    "\n",
    "- Our initial dimensions were (batch, context_length, number of attn heads, dim of the heads)\n",
    "- We want to swap the context length with the number of attn, heads so:\n",
    "  - (batch, number of attn heads, context_length, dim of the heads)\n",
    "- If you cannot see why this is happening now, we will cover it in a second.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = keys.transpose(1, 2)\n",
    "queries = queries.transpose(1, 2)\n",
    "values = values.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the reason we need to take the transpose is so we compute the attn scores, and allow us to take advantage of PyTorch's broadcasting functionality. [What is broadcasting?](https://pytorch.org/docs/stable/notes/broadcasting.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries.shape, keys.transpose(2,3).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we are going to compute the attention scores like we did previosly with matrix mulitplication. Note that we are broadcasting the multiplication across the first two dimensions, so realy we are doing 8x4 (queries) and 4x8 (keys) along the two dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = queries @ keys.transpose(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like before, we need to normalize the attn_scores into attention weights. As you can see the matricies are not normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores[0,0,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first we need to apply the attention masking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_bool = mha.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "# apply the attention masking in place\n",
    "attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "print(\"mask applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores[0,0,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets apply the softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights[0,0,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now dropout!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = mha.dropout(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights[0,0,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before we now need to take the attention weights and combine them with our value vectors.\\\n",
    "\\\n",
    "As we can see, we are still broadcasting across the first two dimensions, so it is like we are performing the following:\\\n",
    "8x8 (attn_weights) @ 8x4 (values) -> 8x4 -> with batch dimension for all heads we have 2x4x8x4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights.shape, values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but we want to use transpose to switch the context_length (in this case 8) and the dimension heads so we can join them together in a later step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape: (b, num_tokens, num_heads, head_dim)\n",
    "context_vec = (attn_weights @ values).transpose(1, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need `.continguous()` here?\n",
    "\n",
    "- checkout this [post](https://stackoverflow.com/questions/48915810/what-does-contiguous-do-in-pytorch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "context_vec = context_vec.contiguous().view(b, num_tokens, mha.d_out)\n",
    "context_vec = mha.o_proj(context_vec) # optional projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we reached the our output for the Multi-headed attention implementation!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Lets Put it All together!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in:int, d_out:int, context_length:int, dropout:float, num_heads=1, bias=False):\n",
    "        super().__init__()\n",
    "        assert(d_out % num_heads == 0), \"The output dimension must be divisible by the number of heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # This reduces the projection dimention to match the desired output.\n",
    "\n",
    "        # Define q, k, v. Remember these...\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=bias)\n",
    "\n",
    "        self.o_proj = nn.Linear(d_out, d_out) # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Create mask for decoder block\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "    \n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys:torch.Tensor = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries:torch.Tensor = self.W_query(x)\n",
    "        values: torch.Tensor = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "       # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.size(-1)**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.o_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, in_dim = 8, 6\n",
    "inputs_v2 = torch.randn((tokens, in_dim))\n",
    "# Create the batch inputs \n",
    "x_batch = torch.stack([inputs_v2, inputs_v2], dim=0)\n",
    "print(f\"Batch Shape: {x_batch.shape}\")\n",
    "print(f\"There are {x_batch.shape[0]} exmaples of size {x_batch.shape[1]}x{x_batch.shape[2]} in the batch\")\n",
    "batch_size, context_length, d_in = x_batch.shape\n",
    "d_out, num_heads = 16, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(d_in, d_out, context_length, dropout=0.1, num_heads=4, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha(x_batch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shared Buffers**\n",
    "\n",
    "- class that is used alongside rope to ruse the attention mask, sin and cos computations each subsequen prediction that improves effeciency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rope import precompute_rope_params\n",
    "class SharedBuffers:\n",
    "    _buffers = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_buffers(context_length, head_dim, rope_base, freq_config, dtype=torch.float32):\n",
    "        # if rope config is not none, get the (mask, cos, sin) config values, otherwise pass none\n",
    "        key = (context_length, head_dim, rope_base, tuple(freq_config.values()) if freq_config else freq_config, dtype)\n",
    "\n",
    "        if key not in SharedBuffers._buffers:\n",
    "            # Create or fetch the buffers\n",
    "            mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "            cos, sin = precompute_rope_params(head_dim, rope_base, context_length, freq_config)\n",
    "            if dtype is not None:\n",
    "                cos = cos.to(dtype)\n",
    "                sin = sin.to(dtype)\n",
    "            SharedBuffers._buffers[key] = (mask, cos, sin)\n",
    "\n",
    "        return SharedBuffers._buffers[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self, d_in, d_out, context_length, num_heads,\n",
    "            num_kv_groups,       # NEW\n",
    "            rope_base=10_000,    # NEW\n",
    "            rope_config=None,    # NEW\n",
    "            dtype=None\n",
    "        ):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        # Set the dimensions of the q, k, v queries\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        # Create the k and v weight matricies. \n",
    "        # Traditionally, the second dim is d_out. If num_kv_groups=1 we have Multi-Query, if num_kv_groups=num_head we have Multi-head attention\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        # query weights are the same as MHA\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtye=dtype)\n",
    "        \n",
    "        # Fetch buffers using Shared buffers class\n",
    "        mask, cos, sin = SharedBuffers.get_buffers(context_length, self.head_dim, rope_base, rope_config, dtype)\n",
    "\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stepping through the forward pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create fake input tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "batch_size = 2\n",
    "context_length = 3000\n",
    "max_context_len=8192\n",
    "embed_dim  =4096\n",
    "num_heads = 36\n",
    "x_batch = torch.randn((batch_size, context_length, embed_dim))\n",
    "# Create the batch inputs \n",
    "print(f\"Batch Shape: {x_batch.shape}\")\n",
    "print(f\"There are {x_batch.shape[0]} exmaples of size {x_batch.shape[1]}x{x_batch.shape[2]} in the batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a sample of input tokens, let's initialize our weight matricies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_kv_groups = 8 # can be tuned, but make sure it evenly divides the number of heads\n",
    "gqa = GroupedQueryAttention(d_in, d_out, context_length, num_heads, num_kv_groups, rope_base=500_000, rope_config=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch.shape #(Batch, number of tokens, input dimension)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
