{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ant-lion/Desktop/Projects/transformers-from-scratch/.venv/lib/python3.12/site-packages/torch/_subclasses/functional_tensor.py:295: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped Query Attention (GQA)\n",
    "\n",
    "A more compute & parameter **efficient** implementation of attention. The idea is to reduce the number of final query groups by reducing the number of KV projections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shared Buffers**\n",
    "\n",
    "- class that is used alongside rope to ruse the attention mask, sin and cos computations each subsequen prediction that improves effeciency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rope import precompute_rope_params_llama3 as precompute_rope_params\n",
    "class SharedBuffers:\n",
    "    _buffers = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_buffers(context_length, head_dim, rope_base, freq_config, dtype=torch.float32):\n",
    "        # if rope config is not none, get the (mask, cos, sin) config values, otherwise pass none\n",
    "        key = (context_length, head_dim, rope_base, tuple(freq_config.values()) if freq_config else freq_config, dtype)\n",
    "\n",
    "        if key not in SharedBuffers._buffers:\n",
    "            # Create or fetch the buffers\n",
    "            mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "            cos, sin = precompute_rope_params(head_dim, rope_base, context_length, freq_config)\n",
    "            if dtype is not None:\n",
    "                cos = cos.to(dtype)\n",
    "                sin = sin.to(dtype)\n",
    "            SharedBuffers._buffers[key] = (mask, cos, sin)\n",
    "\n",
    "        return SharedBuffers._buffers[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self, d_in, d_out, context_length, num_heads,\n",
    "            num_kv_groups,       # NEW\n",
    "            rope_base=10_000,    # NEW\n",
    "            rope_config=None,    # NEW\n",
    "            dtype=None\n",
    "        ):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
    "\n",
    "        # Set the dimensions of the q, k, v queries\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        # Create the k and v weight matricies. \n",
    "        # Traditionally, the second dim is d_out. If num_kv_groups=1 we have Multi-Query, if num_kv_groups=num_head we have Multi-head attention\n",
    "        self.W_key = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, num_kv_groups * self.head_dim, bias=False, dtype=dtype)\n",
    "        self.num_kv_groups = num_kv_groups\n",
    "        self.group_size = num_heads // num_kv_groups\n",
    "\n",
    "        # query weights are the same as MHA\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)\n",
    "        \n",
    "        # Fetch buffers using Shared buffers class\n",
    "        mask, cos, sin = SharedBuffers.get_buffers(context_length, self.head_dim, rope_base, rope_config, dtype)\n",
    "\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stepping through the forward pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create fake input tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape: torch.Size([2, 3000, 4096])\n",
      "There are 2 exmaples of size 3000x4096 in the batch\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "batch_size = 2\n",
    "context_length = 3000\n",
    "max_context_len=8192\n",
    "embed_dim  =4096\n",
    "num_heads = 32\n",
    "x_batch = torch.randn((batch_size, context_length, embed_dim))\n",
    "# Create the batch inputs \n",
    "print(f\"Batch Shape: {x_batch.shape}\")\n",
    "print(f\"There are {x_batch.shape[0]} exmaples of size {x_batch.shape[1]}x{x_batch.shape[2]} in the batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets initialize all of the layers using the GroupedQueryAttention class we created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_kv_groups = 8 # can be tuned, but make sure it evenly divides the number of heads\n",
    "gqa = GroupedQueryAttention(\n",
    "    d_in=embed_dim, \n",
    "    d_out=embed_dim, \n",
    "    context_length=context_length,\n",
    "    num_heads=num_heads,\n",
    "    num_kv_groups=num_kv_groups,\n",
    "    rope_base=500_000\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all of our layers, lets step through the forward pass to see what is happening\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3000, 4096])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First get the dimensions of the input, in this case it is x_batch\n",
    "b, num_tokens, d_in = x_batch.shape\n",
    "x_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the Q, K, and V weight matricies**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = gqa.W_query(x_batch)\n",
    "keys = gqa.W_key(x_batch)\n",
    "values = gqa.W_value(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([2, 3000, 4096])\n",
      "Output Shape: torch.Size([2, 3000, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input Shape: {x_batch.shape}\")\n",
    "print(f\"Output Shape: {queries.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reshape the queries, keys and values matricies**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = queries.view(b, num_tokens, gqa.num_heads, gqa.head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape: torch.Size([2, 3000, 32, 128])\n",
      "See how we split the embedding dimension 4096 into two 32 x 128\n"
     ]
    }
   ],
   "source": [
    "print(f\"Output Shape: {queries.shape}\")\n",
    "print(f\"See how we split the embedding dimension 4096 into two 32 x 128\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do the same for the keys and the values. One thing to notice here is that we are using the number of key value groups here instead of the number of heads. This compresses the latent space of the key and value vectors, which reduces the load on the KV cache, making the attention head much more efficient. There is a trade off between the number of groups.\n",
    "If the number of groups is equivalent to the number of heads we don't see any efficiency gains, but the attention mechanism is more accurate. This is the same as Multiheaded attention [paper]()\n",
    "\n",
    "If the number of groups is 1, we get much better efficiency but an accuracy loss is often present. This is the same as Multi Query attention [paper]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = keys.view(b, num_tokens, gqa.num_kv_groups, gqa.head_dim)\n",
    "values = values.view(b, num_tokens, gqa.num_kv_groups, gqa.head_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transposing Tensors to split the combined heads into their individual heads**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = keys.transpose(1, 2) # (batch, num_kv_groups, num_tokens, head_dim)\n",
    "values = values.transpose(1, 2) # (batch, num_kv_groups, num_tokens, head_dim)\n",
    "queries = queries.transpose(1, 2) # (batch, num_heads, num_tokens, head_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply RoPE**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rope import compute_rope\n",
    "keys = compute_rope(keys, gqa.cos, gqa.sin)\n",
    "queries = compute_rope(queries, gqa.cos, gqa.sin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expand the compressed key and value groups to match the number of heads**\n",
    "\n",
    "This is what is happening here:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
