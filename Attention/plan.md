## Plan for the Posts

### Attention head shorts

- Introduce the attention head
- Show where it is in the architeture (highlighted image)
- Explain the initialization of the attention head
- Step through the forward pass step-by-step.
  - Take a few basic sentences (batches)
  - Show how it is tokenized then transformed into embeddings
  - Show how it is initially passed to the attention head
- Wrap it all together

- Bonus: Create a Manim scene showing the differences in the heads
