{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Llama2 3B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Normalization**\n",
    "\n",
    "One of the main divergences from the original GPT transformer architecture is the normalization technique. The purpose of normalization is to recenter the model weights and inputs to help stablize training and boost model convergence.\n",
    "\n",
    "Traditional **LayerNorm** normalizes the layers using the mean of variance across the feature dimension. There are three hyperparameters $\\epsilon$, setting a floor for the fraction denominator, $\\gamma$ a learnable scaling parameter and $\\beta$ applying a learnable shift parameter.\n",
    "$$y=\\frac{x-E[x]}{\\sqrt{Var(x) + \\epsilon}} * \\gamma + \\beta$$\n",
    "\n",
    "In the LLama2 architecture, **RMSNorm** is used (root mean square normalization). The main benefit of using RMSNorm is that it is more efficient than LayerNorm and it's performance decreases are neglible in practice. Notably, the hyperparameter $\\beta$ is not used in RMSNorm\n",
    "\n",
    "$$y_i=\\frac{x_i}{\\sqrt{\\epsilon +\\frac{1}{n}\\sum{x_i^2}}}*\\gamma_i$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, embd_dim:int, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps # epsilon\n",
    "        self.embd_dim = embd_dim\n",
    "        self.weight = nn.Parameter(torch.ones(embd_dim)).float()\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        means = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        x_norm = x * torch.rsqrt(means + self.eps)\n",
    "        return (x_norm * self.weight).to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward Pass Step Through\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4365, 0.5728, 0.3160, 0.7362, 0.0550, 0.2335, 0.0010, 0.3170],\n",
       "        [0.2950, 0.1941, 0.4875, 0.4818, 0.1934, 0.6766, 0.4779, 0.0472],\n",
       "        [0.0565, 0.3778, 0.6870, 0.1934, 0.3055, 0.6714, 0.5032, 0.8174],\n",
       "        [0.4360, 0.7093, 0.9083, 0.5762, 0.0884, 0.0227, 0.2693, 0.3611]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.rand((4, 8))\n",
    "rmsn = RMSNorm(embd_dim=8)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.6681, 2.8536, 3.6122, 3.3713])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Current unnormalized sum\n",
    "inputs.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1648],\n",
       "        [0.1651],\n",
       "        [0.2651],\n",
       "        [0.2577]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the mean along the feature dimension\n",
    "inputs_mean = inputs.pow(2).mean(dim=1, keepdim=True)\n",
    "inputs_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4060],\n",
       "        [0.4063],\n",
       "        [0.5149],\n",
       "        [0.5076]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add epsilon and take sqrt\n",
    "torch.sqrt(inputs_mean + rmsn.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.4630],\n",
       "        [2.4613],\n",
       "        [1.9422],\n",
       "        [1.9699]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But since we have x / RMS(x) lets take the reciprocal root\n",
    "torch.rsqrt(inputs_mean + rmsn.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0752, 1.4109, 0.7782, 1.8134, 0.1354, 0.5751, 0.0025, 0.7809],\n",
       "        [0.7261, 0.4779, 1.2000, 1.1860, 0.4759, 1.6655, 1.1763, 0.1161],\n",
       "        [0.1097, 0.7339, 1.3342, 0.3756, 0.5934, 1.3039, 0.9774, 1.5875],\n",
       "        [0.8589, 1.3973, 1.7893, 1.1350, 0.1741, 0.0447, 0.5304, 0.7114]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now multiply by the numerator x (inputs in this case) \n",
    "inputs_norm = inputs * torch.rsqrt(inputs_mean + rmsn.eps)\n",
    "inputs_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0752, 1.4109, 0.7782, 1.8134, 0.1354, 0.5751, 0.0025, 0.7809],\n",
       "        [0.7261, 0.4779, 1.2000, 1.1860, 0.4759, 1.6655, 1.1763, 0.1161],\n",
       "        [0.1097, 0.7339, 1.3342, 0.3756, 0.5934, 1.3039, 0.9774, 1.5875],\n",
       "        [0.8589, 1.3973, 1.7893, 1.1350, 0.1741, 0.0447, 0.5304, 0.7114]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, multiply by gamma_i, which are the learnable weights\n",
    "inputs_norm * rmsn.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0752, 1.4109, 0.7782, 1.8134, 0.1354, 0.5751, 0.0025, 0.7809],\n",
       "        [0.7261, 0.4779, 1.2000, 1.1860, 0.4759, 1.6655, 1.1763, 0.1161],\n",
       "        [0.1097, 0.7339, 1.3342, 0.3756, 0.5934, 1.3039, 0.9774, 1.5875],\n",
       "        [0.8589, 1.3973, 1.7893, 1.1350, 0.1741, 0.0447, 0.5304, 0.7114]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rms = RMSNorm(inputs.size(-1))\n",
    "rms.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Activation Changes**\n",
    "\n",
    "Activation functions are non-linear functions connection linear layers in a neural network. Without the non-linear connection neural networks would only learn linear relationships. For example, if I have two matrices - without the activation function of course - and multiply them together, the output matrix would be equivalent to the first two layers, meaning the two layers would collapse into one.\n",
    "\n",
    "There are a variety of activation functions, a simple one being ReLU which essentially clips any negative number to 0, leaving the positive numbers be. This became popular as it helps to circumvent the _vanishing gradients_ issue in NN weights. Recently modern activation functions like GeLU and SiLU, which have smoother approximations, perform better than ReLU. The smoothness in these modern activtation functions allow for more nuanced learning since any negative number isn't immediately cuttoff to 0, like ReLU.\n",
    "\n",
    "Llama 2 uses **SiLU** or sigmoid-weighted linear units.\n",
    "\n",
    "$$silu(x) = x * \\sigma(x)$$\n",
    "\n",
    "Using PyTorch, SiLU is simply implemented below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiLU()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.SiLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feed Forward Layer**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
