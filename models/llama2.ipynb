{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Llama2 3B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Normalization**\n",
    "\n",
    "One of the main divergences from the original GPT transformer architecture is the normalization technique. The purpose of normalization is to recenter the model weights and inputs to help stablize training and boost model convergence.\n",
    "\n",
    "Traditional **LayerNorm** normalizes the layers using the mean of variance across the feature dimension. There are three hyperparameters $\\epsilon$, setting a floor for the fraction denominator, $\\gamma$ a learnable scaling parameter and $\\beta$ applying a learnable shift parameter.\n",
    "$$y=\\frac{x-E[x]}{\\sqrt{Var(x) + \\epsilon}} * \\gamma + \\beta$$\n",
    "\n",
    "In the LLama2 architecture, **RMSNorm** is used (root mean square normalization). The main benefit of using RMSNorm is that it is more efficient than LayerNorm and it's performance decreases are neglible in practice. Notably, the hyperparameter $\\beta$ is not used in RMSNorm\n",
    "\n",
    "$$y_i=\\frac{x_i}{\\sqrt{\\epsilon +\\frac{1}{n}\\sum{x_i^2}}}*\\gamma_i$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, embd_dim:int, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps # epsilon\n",
    "        self.embd_dim = embd_dim\n",
    "        self.weight = nn.Parameter(torch.ones(embd_dim)).float()\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        means = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        x_norm = x * torch.rsqrt(means + self.eps)\n",
    "        return (x_norm * self.weight).to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward Pass Step Through\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand((4, 8))\n",
    "rmsn = RMSNorm(embd_dim=8)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current unnormalized sum\n",
    "inputs.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the mean along the feature dimension\n",
    "inputs_mean = inputs.pow(2).mean(dim=1, keepdim=True)\n",
    "inputs_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add epsilon and take sqrt\n",
    "torch.sqrt(inputs_mean + rmsn.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But since we have x / RMS(x) lets take the reciprocal root\n",
    "torch.rsqrt(inputs_mean + rmsn.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now multiply by the numerator x (inputs in this case) \n",
    "inputs_norm = inputs * torch.rsqrt(inputs_mean + rmsn.eps)\n",
    "inputs_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, multiply by gamma_i, which are the learnable weights\n",
    "inputs_norm * rmsn.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = RMSNorm(inputs.size(-1))\n",
    "rms.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Activation Functions**\n",
    "\n",
    "Activation functions are non-linear functions connection linear layers in a neural network. Without the non-linear connection neural networks would only learn linear relationships. For example, if I have two matrices - without the activation function of course - and multiply them together, the output matrix would be equivalent to the first two layers, meaning the two layers would collapse into one.\n",
    "\n",
    "There are a variety of activation functions, a simple one being ReLU which essentially clips any negative number to 0, leaving the positive numbers be. This became popular as it helps to circumvent the _vanishing gradients_ issue in NN weights. Recently modern activation functions like GeLU and SiLU, which have smoother approximations, perform better than ReLU. The smoothness in these modern activtation functions allow for more nuanced learning since any negative number isn't immediately cuttoff to 0, like ReLU.\n",
    "\n",
    "Llama 2 uses **SwiLU** or a Gate linear unit variant of sigmoid-weighted linear units.\n",
    "\n",
    "**SiLU**\n",
    "\n",
    "$$silu(x) = x * \\sigma(x)$$\n",
    "\n",
    "**SwiGLU**\n",
    "\n",
    "$$SwiGLU(x) = SiLU(Linear_1(x)) * Linear_2(x)$$\n",
    "\n",
    "Using PyTorch, SiLU is simply implemented below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.SiLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feed Forward Layer**\n",
    "\n",
    "The feed forward layer is where the network gets to \"think about\" each token individually.\n",
    "\n",
    "In the feed forward network, Llama 2 uses **SwiLU** or a Gate linear unit variant of sigmoid-weighted linear units.\n",
    "\n",
    "**Recall SiLU is**:\n",
    "\n",
    "$$silu(x) = x * \\sigma(x)$$\n",
    "\n",
    "**SwiGLU**:\n",
    "\n",
    "$$SwiGLU(x) = SiLU(Linear_1(x)) * Linear_2(x)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config:dataclass):\n",
    "        super().__init__()\n",
    "        # 3 Linear layers, one silu in between the two\n",
    "        self.fc1 = nn.Linear(config.emb_dim, config.hidden_dim, dtype=config.dtype, bias=config.ff_bias)\n",
    "        self.fc2 = nn.Linear(config.emb_dim, config.hidden_dim, dtype=config.dtype, bias=config.ff_bias)\n",
    "        self.fc3 = nn.Linear(config.hidden_dim, config.emb_dim, dtype=config.dtype, bias=config.ff_bias)\n",
    "        self.silu = nn.SiLU()\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = self.silu(x_fc1) * x_fc2\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward Pass Step Through\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ExConfig:\n",
    "    emb_dim = 8\n",
    "    hidden_dim = 4\n",
    "    ff_bias = False\n",
    "    dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.stack([torch.rand(4, 8), torch.rand(4, 8)], dim=0)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = FeedForward(ExConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = ff.fc1(inputs)\n",
    "fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc2 = ff.fc2(inputs)\n",
    "fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice how it balanced out the weights.\n",
    "silu = ff.silu(fc1)\n",
    "silu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = silu * fc2\n",
    "out = ff.fc3(x)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(ff.forward(inputs), out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Posistional Encodings**\n",
    "\n",
    "Another divergence from the original GPT architecture is in the positional encoing approach.\n",
    "There are two main token positions to contend with, _absolute_ and _relative_.\n",
    "\n",
    "- _Absolute_ position is the token of interest's position within the whole sequence of tokens\n",
    "\n",
    "- _Relative_ position is the token of interest's postition compared to another token within the sequence.\n",
    "\n",
    "**Sinusoidal Positional Encoding**\n",
    "\n",
    "- The original tranformer architecture used sinusoidal posistional encoding, which creates frequencies vector, using sine and cosine, to communicate a token's position.\n",
    "- The downside of this approach is encodings start to break down when the model encounters sequence lengths longer than those it was trained on.\n",
    "\n",
    "**Rotary Positional Embeddings (RoPE)**\n",
    "\n",
    "- RoPE is a modern approach to positional encoding. RoPE applies rotations to represent a tokens positional embedding. Unlike sinusoidal positional encoding, RoPE can better extend to longer contexts not seen in training because of it's rotational properties.\n",
    "\n",
    "_The equation is:_\n",
    "\n",
    "$$f_{\\{q,k\\}}(x_m, m) = R^d_{\\Theta,m}W_{\\{q, k\\}}x_m$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_rope_params(head_dim, theta_base=10_000, context_length=4096):\n",
    "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Compute the inverse frequencies\n",
    "    inv_freq = 1.0 / ( theta_base ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))\n",
    "\n",
    "    # Generate position indices\n",
    "    positions = torch.arange(context_length)\n",
    "\n",
    "    # Compute the angles\n",
    "    angles = positions[:, None] * inv_freq[None, :]  # Shape: (context_length, head_dim // 2)\n",
    "\n",
    "    # Expand angles to match the head_dim\n",
    "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
    "\n",
    "    # Precompute sine and cosine\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    return cos, sin\n",
    "\n",
    "def compute_rope(x, cos, sin):\n",
    "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
    "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
    "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "\n",
    "    # Split x into first half and second half\n",
    "    x1 = x[..., : head_dim // 2]  # First half\n",
    "    x2 = x[..., head_dim // 2 :]  # Second half\n",
    "\n",
    "    # Apply the rotary transformation\n",
    "    rotated = torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "    # Adjust sin and cos shapes\n",
    "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
    "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    x_rotated = (x * cos) + (rotated * sin)\n",
    "    return x_rotated.to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Steping Through the Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the settings\n",
    "batch_size, context_len, num_heads, head_dim, theta_base = 2, 5, 4, 16, 10_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore `precompute_rope_params()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first make sure that head dim is an event number\n",
    "assert head_dim % 2 == 0, \"Head dimension needs to be even\"\n",
    "# Create a tensor from 0 -> head_dim counting by 2\n",
    "torch.arange(0, head_dim, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert values into floats, and divide by head_dim to normalize frequencies\n",
    "torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply by theta_base to control how quickly the frequencies decay\n",
    "theta_base ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The whole thing put together\n",
    "inv_freq = 1.0 / ( theta_base ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))\n",
    "inv_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the inverse frequencies, lets generate position indicies. This is a simple list of numbers from 0 - `context_length`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = torch.arange(context_len)\n",
    "positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding another dimension to the positions and frequencies\n",
    "print(f\"Previous Position Shape: {positions.shape}\\nNew Position Shape: {positions[:, None].shape}\")\n",
    "print(\"=\"*5)\n",
    "print(f\"Previous Inverse Frequency Shape: {inv_freq.shape}\\nNew Inverse Frequency Shape: {inv_freq[None, :].shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can add a new dimension another way by using unsqueeze\n",
    "positions.unsqueeze(dim=-1), inv_freq.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either way you choose to insert a new dimension is up to you.\n",
    "# Once you insert a new dimension we do element wize multiplication\n",
    "angles = (positions[:, None] * inv_freq[None, :])\n",
    "angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets expand the we need to join angles with itself since we only have head_dim // 2\n",
    "angles = torch.cat([angles, angles], dim=1)\n",
    "angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sine and cosine. This is fairly straight forward\n",
    "cos = torch.cos(angles)\n",
    "sin = torch.sin(angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corresponding sin and cos tensors, we can compute RoPE from the `compute_rope()` function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECALL: batch_size, context_len, num_heads, head_dim, theta_base = 2, 5, 4, 16, 10_000\n",
    "# Create dummy query and key tensors\n",
    "queries = torch.randn(batch_size, num_heads, context_len, head_dim)\n",
    "keys = torch.randn(batch_size, num_heads, context_len, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries[..., :head_dim//2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the queries and keys of dimension 2x4x5x16. Recall from the attention [notebook](../Attention/play.ipynb), we are dealing with weight splitting. So what does this mean?\n",
    "\n",
    "- The first dimension is the batch dimension or the number of token sequences we are passing through the model.\n",
    "- The second dimension is the number of attention heads we have. This is where weight splitting comes into play. Since we have 4 attention heads, we actually package all of the query and key vectors in one tensor. By splitting along this dimension we get 4 separate _context_len x head_dim_ matricies.\n",
    "- The third dimension is the context length or the number of tokens in the sequence that is passed in to the transformer.\n",
    "- The fourth dimension is the dimension of each attention head. This also represents the number of features that are in a Q, K, and V vector. Remember there is one Q, K, V vector to represent each token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get the dimensions of our input x. x is either the query or key vectors\n",
    "batch_size, num_heads, seq_len, head_dim = queries.shape\n",
    "# ensure the head_dim is even\n",
    "assert head_dim % 2 == 0, \"Head dimension must be even\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the code below. What this code is doing is taking the original query vector, splitting the embeddings for each token in half, negating the original second half and then concatonating the negated, negative second half with the orignal first half. This is a mathematical trick that emulates multiplication by the original rotary embedding matrix.\n",
    "\n",
    "lets have a look at the 2D rotational matrix.\n",
    "\n",
    "\\begin{pmatrix}\n",
    "\n",
    "cos(m\\theta) & -sin(m\\theta) \\\\\n",
    "\n",
    "cos(m\\theta) & sin(m\\theta)\n",
    "\n",
    "\\end{pmatrix}\n",
    "\n",
    "Now lets take a look at what the code below is doing with the following example:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "\n",
    "5 & 6 & 7 & 8 \\\\\n",
    "\n",
    "1 & 4 & 5 & 9\n",
    "\n",
    "\\end{pmatrix}\n",
    "=>\n",
    "\\begin{pmatrix}\n",
    "\n",
    "1 & 2 \\\\\n",
    "\n",
    "5 & 6 \\\\\n",
    "\n",
    "1 & 4\n",
    "\n",
    "\\end{pmatrix}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{pmatrix}\n",
    "\n",
    "-3 & -4 \\\\\n",
    "\n",
    "-7 & -8 \\\\\n",
    "\n",
    "-5 & -9\n",
    "\n",
    "\\end{pmatrix}\n",
    "\n",
    "=>\n",
    "\n",
    "\\begin{pmatrix}\n",
    "\n",
    "-3 & -4 & 1 & 2 \\\\\n",
    "\n",
    "-7 & -8 & 5 & 6 \\\\\n",
    "\n",
    "-5 & -9 & 1 & 4\n",
    "\n",
    "\\end{pmatrix}\n",
    "\n",
    "\n",
    "$$\n",
    "\n",
    "The reasoning may be confusing at the moment, but I will show how this is advantageous shortly. Remeber the number of rows are the sequence legnth, and the columns are the embeddings. This means each row represents query embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the query into its first and second halfs\n",
    "q_first = queries[..., : head_dim // 2]\n",
    "q_second = queries[..., head_dim // 2 : ]\n",
    "rotated = torch.cat([q_first, q_second], dim=-1)\n",
    "print(f\"Full Query Shape: {queries.shape}\")\n",
    "print(f\"First half Query Shape: {q_first.shape}\")\n",
    "print(f\"Second half Query Shape: {q_second.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We spoke about broadcasting, a very important PyTorch concept, in the attention [notebook](\"../Attention/play.ipynb\"). In order to properly apply the cos and sin transformations we need to create two new dimensions of size 1 for proper broadcasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust sin and cos shapes\n",
    "# Option 1: cos[:seq_len, :][None, None, :, :].shape\n",
    "# Option 2:\n",
    "print(f\"Previous Cos shape: {cos.shape}\")\n",
    "cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
    "print(f\"New Cos shape: {cos.shape}\")\n",
    "# apply the same for sin\n",
    "sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to explain what is going on in the next code block, lets recall what we did with the previous embedding split and negation:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "\n",
    "5 & 6 & 7 & 8 \\\\\n",
    "\n",
    "1 & 4 & 5 & 9\n",
    "\n",
    "\\end{pmatrix}\n",
    "=(split)=>\n",
    "\\begin{pmatrix}\n",
    "\n",
    "1 & 2 \\\\\n",
    "\n",
    "5 & 6 \\\\\n",
    "\n",
    "1 & 4\n",
    "\n",
    "\\end{pmatrix}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{pmatrix}\n",
    "\n",
    "-3 & -4 \\\\\n",
    "\n",
    "-7 & -8 \\\\\n",
    "\n",
    "-5 & -9\n",
    "\n",
    "\\end{pmatrix}\n",
    "\n",
    "=(rotate)=>\n",
    "\\begin{pmatrix}\n",
    "\n",
    "-3 & -4 & 1 & 2 \\\\\n",
    "\n",
    "-7 & -8 & 5 & 6 \\\\\n",
    "\n",
    "-5 & -9 & 1 & 4\n",
    "\n",
    "\\end{pmatrix}\n",
    "\n",
    "\n",
    "$$\n",
    "\n",
    "Now we take the queries, do component wise multiplication using the RoPE cos frequencies and sin frequencies\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "\n",
    "5 & 6 & 7 & 8 \\\\\n",
    "\n",
    "1 & 4 & 5 & 9\n",
    "\n",
    "\\end{pmatrix}\n",
    "* cos\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "\n",
    "-3 & -4 & 1 & 2 \\\\\n",
    "\n",
    "-7 & -8 & 5 & 6 \\\\\n",
    "\n",
    "-5 & -9 & 1 & 4\n",
    "\n",
    "\\end{pmatrix}\n",
    "* sin\n",
    "$$\n",
    "\n",
    "Although this does not look exactly the same as the RoPE equation below, it is a mathematical trick that acheives the same geometric rotation more efficiently:\n",
    "\n",
    "$$f_{\\{q,k\\}}(x_m, m) = R^d_{\\Theta,m}W_{\\{q, k\\}}x_m$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_rotated = ( queries * cos ) + ( rotated * sin )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multi-Headed Attention**\n",
    "\n",
    "Multiheaded attention was covered extensively in this notebook [here](\"../Attention/play.ipynb\")\n",
    "\n",
    "But let's implement Attention applying RoPE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, num_heads, dtype=None):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"The output dimension (d_out) mus be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads \n",
    "\n",
    "        # Set q, k, v, vectors and output projection\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
    "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)\n",
    "\n",
    "        # Create token masking \n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "        # Create RoPE parameters\n",
    "        cos, sin = precompute_rope_params(head_dim=self.head_dim, context_length=context_length)\n",
    "        self.register_buffer(\"cos\", cos)\n",
    "        self.register_buffer(\"sin\", sin)\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # Calculate the q, k, v vectors\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Remember the output dim is all of the q, k, v vectors concatonated. Need to split\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Now we need to swap the head dimension and the context dim\n",
    "        queries:torch.Tensor = queries.transpose(1,2)\n",
    "        keys:torch.Tensor = keys.transpose(1,2)\n",
    "        values:torch.Tensor = values.transpose(1,2)\n",
    "\n",
    "        # compute the positional encodings for the key and the values\n",
    "        keys = compute_rope(keys, self.cos, self.sin)\n",
    "        queries = compute_rope(queries, self.cos, self.sin)\n",
    "\n",
    "        # Attention scores via scaled dot-product\n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "\n",
    "        # Masking\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use mask to fill upper triangle of attention to -inf for norm\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        # normalizing the weights\n",
    "        attn_weights = torch.softmax(attn_scores / keys.size(-1)**0.5, dim=-1)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1,2)\n",
    "\n",
    "        # combine the heads again self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # projection\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "batch_size = 1\n",
    "context_len = 100\n",
    "max_context_len = 4096\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "\n",
    "example_batch = torch.randn((batch_size, context_len, embed_dim))\n",
    "\n",
    "mha = MultiHeadedAttention(d_in=embed_dim, d_out=embed_dim, context_length=max_context_len, num_heads=num_heads)\n",
    "mha.forward(example_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg:dataclass):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadedAttention(\n",
    "            d_in=cfg.emb_dim,\n",
    "            d_out=cfg.emb_dim,\n",
    "            context_length=cfg.context_length,\n",
    "            num_heads=cfg.n_heads,\n",
    "            dtype=cfg.dtype\n",
    "        )\n",
    "\n",
    "        self.ff = FeedForward(cfg)\n",
    "\n",
    "        # RMSNorms\n",
    "        self.norm1 = RMSNorm(cfg.emb_dim)\n",
    "        self.norm2 = RMSNorm(cfg.emb_dim)\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        # save x for the residual connections \n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = x + residual\n",
    "\n",
    "        # reset residual connections\n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = x + residual\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Llama2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama2(nn.Module):\n",
    "    def __init__(self, cfg:dataclass):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg.vocab_size, cfg.emb_dim, dtype=cfg.dtype)\n",
    "\n",
    "        # Transformer block\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "\n",
    "        # RMSNorm\n",
    "        self.final_norm = RMSNorm(cfg.emb_dim)\n",
    "\n",
    "        # Final output layer\n",
    "        self.out_head = nn.Linear(cfg.emb_dim, cfg.vocab_size, bias=False, dtype=cfg.dtype)\n",
    "    \n",
    "    def forward(self,  in_idx:torch.Tensor):\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        x = tok_embeds \n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Llama2Config7B:\n",
    "    vocab_size = 32000      # Vocabulary size\n",
    "    context_length = 4096   # Context length\n",
    "    emb_dim = 4096          # Embedding dimension\n",
    "    n_heads = 32            # Number of attention heads\n",
    "    ff_bias = False         # Feed forward bias \n",
    "    n_layers = 32           # Number of layers\n",
    "    hidden_dim = 11008      # NEW = Size of the intermediate dimension in FeedForward\n",
    "    dtype = torch.bfloat16  # NEW: Lower-precision dtype to reduce memory usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Llama2(Llama2Config7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_memory_size(model, input_dtype=torch.float32):\n",
    "    total_params = 0\n",
    "    total_grads = 0\n",
    "    for param in model.parameters():\n",
    "        # Calculate total number of elements per parameter\n",
    "        param_size = param.numel()\n",
    "        total_params += param_size\n",
    "        # Check if gradients are stored for this parameter\n",
    "        if param.requires_grad:\n",
    "            total_grads += param_size\n",
    "\n",
    "    # Calculate buffer size (non-parameters that require memory)\n",
    "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
    "\n",
    "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
    "    # We assume parameters and gradients are stored in the same type as input dtype\n",
    "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
    "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
    "\n",
    "    # Convert bytes to gigabytes\n",
    "    total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "    return total_memory_gb\n",
    "\n",
    "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
    "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "login(token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "tokenizer_file = hf_hub_download(\n",
    "    repo_id=\"meta-llama/Llama-2-7b\",\n",
    "    filename=\"tokenizer.model\",\n",
    "    local_dir=\"Llama-2-7b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "class LlamaTokenizer:\n",
    "    def __init__(self, tokenizer_file):\n",
    "        sp = spm.SentencePieceProcessor()\n",
    "        sp.load(tokenizer_file)\n",
    "        self.tokenizer = sp\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.tokenizer.encode_as_ids(text)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return self.tokenizer.decode_pieces(ids)\n",
    "\n",
    "\n",
    "tokenizer = LlamaTokenizer(tokenizer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama2_util import generate, token_ids_to_text, text_to_token_ids\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves\", tokenizer).to(device),\n",
    "    max_new_tokens=30,\n",
    "    context_size=Llama2Config7B.context_length\n",
    "    top_k=1,\n",
    "    temperature=0.\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
